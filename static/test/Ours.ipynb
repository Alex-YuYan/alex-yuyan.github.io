{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChuanyangJin\\AppData\\Local\\Temp\\ipykernel_3252\\2079687195.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  texts[:] = [text.lower() for text in texts]                                 # convert into lower letters\n",
      "C:\\Users\\ChuanyangJin\\AppData\\Local\\Temp\\ipykernel_3252\\2079687195.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  texts[:] = [re.sub(r'https?:\\/\\/.*\\/\\w*', 'URL', text) for text in texts]   # http:// ... / word  or  http:// ... / word\n",
      "C:\\Users\\ChuanyangJin\\AppData\\Local\\Temp\\ipykernel_3252\\2079687195.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  texts[:] = [re.sub(r'@\\w+([-.]\\w+)*', '', text) for text in texts]          # meaningless things like @AlexYan\n",
      "C:\\Users\\ChuanyangJin\\AppData\\Local\\Temp\\ipykernel_3252\\2079687195.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  texts[:] = [re.sub(r'&\\w+([-.]\\w+)*', '', text) for text in texts]          # meaningless things like &amp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Reading in the dataset\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Clean the text data\n",
    "def clean_text(texts):\n",
    "    texts[:] = [text.lower() for text in texts]                                 # convert into lower letters\n",
    "    texts[:] = [re.sub(r'https?:\\/\\/.*\\/\\w*', 'URL', text) for text in texts]   # http:// ... / word  or  http:// ... / word\n",
    "    texts[:] = [re.sub(r'@\\w+([-.]\\w+)*', '', text) for text in texts]          # meaningless things like @AlexYan\n",
    "    texts[:] = [re.sub(r'&\\w+([-.]\\w+)*', '', text) for text in texts]          # meaningless things like &amp\n",
    "\n",
    "clean_text(train_df['text'])\n",
    "clean_text(test_df['text'])\n",
    "\n",
    "# print(train_df.head)\n",
    "# no.       Before                                                  After\n",
    "# 0         Our Deeds are the Reason of this #earthquake M...       our deeds are the reason of this #earthquake m...\n",
    "# 7069      @aria_ahrary @TheTawniest The out of control w...       the out of control wild fires in california ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attemp: using sklearn.feature_extraction.text.CountVectorizer()\n",
    "# This converts a collection of text documents to a matrix of token counts,\n",
    "# and produces a sparse representation of the counts using scipy.sparse.csr_matrix.\n",
    "\n",
    "# It produces 14717 features, corresponding to the 14717 distinct words in training data.\n",
    "# However, the number of features is too large to us, considering our training sample size is also large,\n",
    "# which slows down the following model training.\n",
    "# Instead, we decide to use a bert model. \n",
    "\n",
    "# from sklearn import feature_extraction\n",
    "# count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "# X_train = count_vectorizer.fit_transform(train_df[\"text\"])\n",
    "# X_test = count_vectorizer.transform(test_df[\"text\"])\n",
    "\n",
    "# print(type(train_vectors))                    # 'scipy.sparse.csr.csr_matrix'\n",
    "# print(train_vectors.shape)                    # (7613, 14717): 7613 training samples, 14717 features\n",
    "# train_array = train_vectors.toarray()\n",
    "# print(train_array)\n",
    "\n",
    "\n",
    "\n",
    "# Second attemp: using a pre-trained bert model\n",
    "# It produces 384 features.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "bert_model = SentenceTransformer('all-MiniLM-L12-v1')\n",
    "X_train = bert_model.encode(train_df[\"text\"])\n",
    "X_test = bert_model.encode(test_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 384)\n",
      "[[ 0.00230695  0.07191736  0.09788872 ...  0.07197042 -0.02740236\n",
      "  -0.07827645]\n",
      " [ 0.02561178  0.06068205 -0.0192754  ... -0.02490251 -0.05799778\n",
      "  -0.00681885]\n",
      " [ 0.11690209  0.04465307  0.08812291 ... -0.01944826 -0.03323479\n",
      "   0.02873739]\n",
      " ...\n",
      " [-0.00110018 -0.00252272  0.06226555 ...  0.04964388 -0.04241561\n",
      "   0.04092398]\n",
      " [-0.04007943  0.06662016  0.0298096  ...  0.07253803 -0.0047584\n",
      "   0.03727422]\n",
      " [ 0.06300642 -0.00216898  0.05352418 ...  0.01271525  0.01957589\n",
      "   0.05859178]]\n",
      "(3263, 384)\n",
      "[[ 1.5842754e-03 -6.8736620e-02  3.9622501e-02 ...  2.9520888e-02\n",
      "   3.6666408e-02  3.1300120e-02]\n",
      " [ 1.2380565e-01 -6.9417410e-02  8.9123756e-02 ...  3.4361023e-02\n",
      "  -3.5430912e-05  1.1519138e-02]\n",
      " [ 1.5148669e-01  6.8411618e-03 -1.6904498e-02 ... -4.8215881e-02\n",
      "  -7.4761413e-02  6.8294503e-02]\n",
      " ...\n",
      " [-6.3210293e-03  2.7175112e-02 -1.4542190e-02 ... -3.1739313e-02\n",
      "  -2.7545594e-02  6.9203213e-02]\n",
      " [ 9.5806429e-03 -1.5488052e-02  5.4098818e-02 ... -6.7440167e-02\n",
      "  -1.1712046e-01  1.1178589e-01]\n",
      " [-1.1412573e-02 -1.9464432e-03  7.5697899e-02 ...  3.1711578e-02\n",
      "   2.6648028e-03 -5.8913771e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)     # (7613, 384): 7613 training samples, 384 features\n",
    "print(X_train)\n",
    "print(X_test.shape)      # (3263, 384): 3263 training samples, 384 features\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set into training and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "y_label = train_df['target']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_label, test_size=0.2, random_state=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1000, max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "# Find the predicted values on the validation set\n",
    "y_hat_logreg = logreg.predict(X_val)\n",
    "# Find the accuracy achieved on the validation set\n",
    "acc_logreg = np.mean(y_hat_logreg == y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prec:  0.8030560271646859\n",
      "recal:  0.7243491577335375\n",
      "fscore:  0.7616747181964573\n"
     ]
    }
   ],
   "source": [
    "# Find Precision, recall and fscore on the validation set\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "prec, recal, fscore, _ = precision_recall_fscore_support(y_val, y_hat_logreg, average='binary')\n",
    "print('prec: ', prec)\n",
    "print('recal: ', recal)\n",
    "print('fscore: ', fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the sample_submission.csv file\n",
    "# Preict\n",
    "submission = pd.read_csv(\"sample_submission.csv\")\n",
    "submission[\"target\"] = logreg.predict(X_test)\n",
    "submission.head()\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ed2c744ac16acf37243a821035b7f5cb63920d4c66e4ef2471e2335116d514e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
